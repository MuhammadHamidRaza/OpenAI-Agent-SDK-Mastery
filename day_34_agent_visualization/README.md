# Day 34: Agent Visualization

[![Proprietary License](https://img.shields.io/badge/license-proprietary-red.svg)](../LICENSE)

---

### **Course Overview**

Welcome to Day 34 of the **OpenAI Agent SDK Mastery** course! We've explored various techniques to build intelligent and efficient agents. Today, we focus on **Agent Visualization**, a powerful capability that transforms the opaque internal workings of an AI agent into clear, understandable visual representations. As agents become more complex, with multiple steps, tool calls, and decision points, visualizing their workflow becomes indispensable for debugging, understanding their reasoning, and effectively communicating their behavior. This session will delve into the benefits of agent visualization and how you can leverage the data provided by the SDK (especially `RunResult` and tracing events) to create insightful visual traces of your agent's journey.

---

## Why Visualize Agent Workflows?

AI agents, particularly those employing complex reasoning patterns like ReAct or multi-agent orchestration, can be challenging to debug and understand. A simple print statement of the final output often doesn't reveal *how* the agent arrived at that answer. Visualization provides:

*   **Enhanced Debugging:** Quickly identify where an agent deviates from expected behavior, gets stuck in loops, or makes incorrect tool calls.
*   **Improved Understanding:** Gain deeper insights into the LLM's thought process, decision-making, and how it interacts with tools and external systems.
*   **Better Communication:** Clearly explain the agent's logic and flow to non-technical stakeholders or team members.
*   **Performance Optimization:** Spot bottlenecks or inefficient paths in the agent's execution.
*   **Trust and Transparency:** Build confidence in the agent's reliability by making its operations transparent.

Visualization transforms the "black box" of an agent into a transparent, inspectable system.

---

## What to Visualize in an Agent Workflow

Effective agent visualizations typically represent the flow of information and control. Key elements to visualize include:

1.  **Nodes (States/Steps):** Represent distinct stages in the agent's process:
    *   Initial Prompt
    *   LLM Thought/Reasoning
    *   Tool Call
    *   Tool Output
    *   Handoff to another agent
    *   Final Answer
2.  **Edges (Transitions):** Arrows connecting nodes, showing the flow of execution from one step to the next.
3.  **Data/Context:** Information passed between steps (e.g., the prompt, tool arguments, tool results, LLM messages).
4.  **Metadata:** Additional information like timestamps, duration of each step, token usage, or which agent is active.

Common visualization types include sequence diagrams, flowcharts, or custom graph representations.

---

## Extracting Data for Visualization from the SDK

The OpenAI Agents SDK provides all the necessary data for visualization through the `RunResult` object, specifically its `new_items` property (as discussed on Day 9 and Day 10). Each `RunItem` in `new_items` represents a discrete event in the agent's execution trace.

### Key `RunItem` Types for Visualization:

*   **`MessageOutputItem`:** Represents text generated by the LLM (thoughts, partial responses, final answers).
*   **`ToolCallItem`:** Indicates that the LLM decided to call a tool, including the tool's name and arguments.
*   **`ToolCallOutputItem`:** Contains the output received from a tool after its execution.
*   **`ReasoningItem`:** (If available/enabled) Provides explicit reasoning steps from the LLM.
*   **`HandoffCallItem` / `HandoffOutputItem`:** For multi-agent systems, shows the delegation of tasks.

By iterating through `result.new_items`, you can reconstruct the entire sequence of events and build a data structure suitable for visualization.

### Example: Extracting Trace Data for a Conceptual Visualization

```python
from agents import Agent, Runner
from agents.tools import WebSearchTool # Assuming this is available
import os

# Ensure API key is set
if "OPENAI_API_KEY" not in os.environ:
    print("Please set the OPENAI_API_KEY environment variable.")
    exit()

# Assume WebSearchTool is configured
web_search_tool = WebSearchTool()

visual_agent = Agent(
    name="VisualAgent",
    instructions="You are a helpful assistant that can answer questions using web search.",
    tools=[web_search_tool]
)

def get_agent_trace_data(agent: Agent, query: str) -> list:
    """Runs an agent and extracts a simplified trace of its execution."""
    print(f"Running agent '{agent.name}' to get trace data for: '{query}'")
    result = Runner.run_sync(agent, query)

    trace_data = []
    trace_data.append({"type": "Input", "content": query})

    for item in result.new_items:
        item_type = type(item).__name__
        if item_type == "MessageOutputItem":
            trace_data.append({"type": "LLM_Message", "content": item.text})
        elif item_type == "ToolCallItem":
            trace_data.append({"type": "Tool_Call", "tool_name": item.tool_name, "tool_args": item.tool_args})
        elif item_type == "ToolCallOutputItem":
            trace_data.append({"type": "Tool_Output", "content": item.output})
        elif item_type == "ReasoningItem":
            trace_data.append({"type": "LLM_Reasoning", "content": item.text})
        # Add other item types as needed (e.g., HandoffCallItem)

    trace_data.append({"type": "Final_Output", "content": result.final_output})
    return trace_data

# Get trace data for a sample query
query = "What is the capital of France and what is its population?"
sample_trace = get_agent_trace_data(visual_agent, query)

print("\n--- Extracted Trace Data (JSON-like structure) ---")
import json
print(json.dumps(sample_trace, indent=2))

# Conceptual Visualization (using the extracted data)
print("\n--- Conceptual Visualization Output ---")
for step in sample_trace:
    if step["type"] == "Input":
        print(f"[START] User Input: {step["content"]}")
    elif step["type"] == "LLM_Message":
        print(f"  [LLM] Message: {step["content"][:50]}...")
    elif step["type"] == "Tool_Call":
        print(f"  [TOOL] Calling {step["tool_name"]}({step["tool_args"]})")
    elif step["type"] == "Tool_Output":
        print(f"  [TOOL] Output: {step["content"][:50]}...")
    elif step["type"] == "LLM_Reasoning":
        print(f"  [LLM] Reasoning: {step["content"][:50]}...")
    elif step["type"] == "Final_Output":
        print(f"[END] Final Output: {step["content"]}")

```

**Explanation:**

*   The `get_agent_trace_data` function runs an agent and then iterates through its `result.new_items`.
*   It extracts relevant information from each `RunItem` and stores it in a structured list of dictionaries (`trace_data`).
*   This `trace_data` can then be used by any visualization library (e.g., `graphviz`, `mermaid.js`, custom D3.js charts) to render a visual representation of the agent's workflow.

---

## Tools and Libraries for Visualization (External)

While the SDK provides the data, you'll typically use external libraries or platforms for actual rendering:

*   **Graph Visualization Libraries:** `Graphviz`, `NetworkX` (Python) for creating node-edge diagrams.
*   **Diagramming Tools:** `Mermaid.js` (for markdown-based diagrams), `PlantUML`.
*   **Observability Platforms:** Dedicated platforms (e.g., LangSmith, Weights & Biases) often provide built-in visualization for agent traces.
*   **Custom UIs:** For highly tailored visualizations, you might build a custom web interface using JavaScript libraries like D3.js or React Flow.

---

## Key Takeaways

*   **Agent Visualization** is crucial for understanding, debugging, and communicating complex AI agent behaviors.
*   It provides transparency into the "black box" of LLM-driven workflows.
*   The OpenAI Agents SDK provides all the necessary trace data through the `RunResult.new_items` property.
*   You can extract this data and use external libraries or platforms to create visual representations of agent inputs, LLM thoughts, tool calls, and outputs.

Today, you've learned how to gain deeper insights into your agent's operations. Tomorrow, we'll explore **SDK Configuration & `RunConfig`**, mastering how to fine-tune the behavior of individual agent runs for even greater control.