# Day 9: `RunResult` and Streaming

[![Proprietary License](https://img.shields.io/badge/license-proprietary-red.svg)](../LICENSE)

---

### **Course Overview**

Welcome to Day 9 of the **OpenAI Agent SDK Mastery** course! We've explored how agents think, act, and remember. Today, we focus on understanding the *outcome* of an agent's execution and how to enhance the user experience by providing real-time feedback. We'll dive deep into the `RunResult` object, which encapsulates all the details of an agent's run, and then explore **streaming**, a powerful technique to deliver partial outputs as they are generated, making your agent applications feel more responsive and dynamic. This session will equip you with the knowledge to inspect agent behavior and build more engaging user interfaces.

---

## Understanding the `RunResult` Object

Whenever you execute an agent using `Runner.run()`, `Runner.run_sync()`, or `Runner.run_streamed()`, the primary output is a `RunResult` object (or `RunResultStreaming` for streamed runs, which inherits from `RunResultBase`). This object is a comprehensive container for everything that happened during the agent's execution. It's your window into the agent's thought process, tool usage, and final outcome.

Inspecting the `RunResult` is crucial for debugging, understanding agent behavior, and extracting the final answer or any intermediate steps.

### Key Properties of `RunResult` (and `RunResultStreaming`):

*   **`final_output`**: This is the most commonly used property, containing the agent's ultimate answer or the final output of the run. It's typically a string, but can be an object if a specific `output_type` was defined for the agent.
*   **`new_items`**: A chronological list of `RunItem` objects that represent every significant event during the agent's execution. This includes:
    *   `MessageOutputItem`: Messages generated by the LLM.
    *   `ToolCallItem`: An indication that the LLM requested a tool to be called.
    *   `ToolCallOutputItem`: The output received from a tool after its execution.
    *   `ReasoningItem`: Insights into the LLM's thought process (if enabled).
    *   `HandoffCallItem` / `HandoffOutputItem`: Related to multi-agent handoffs (covered later).
*   **`last_agent`**: The `Agent` instance that produced the `final_output`. Useful in multi-agent scenarios to know which agent completed the task.
*   **`input`**: The original input provided to the `run` method.
*   **`to_input_list()`**: A utility method that converts the run's history into a format suitable for continuing the conversation manually (without sessions).
*   **`raw_responses`**: Low-level `ModelResponses` from the LLM, containing token information and metadata.
*   **`input_guardrail_results` / `output_guardrail_results`**: Results from any guardrails applied to the input or output (covered later).

### Example: Inspecting `RunResult`

```python
from agents import Agent, Runner
from agents import function_tool
import os

# Ensure the OpenAI API key is set
# os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY" 

if "OPENAI_API_KEY" not in os.environ:
    print("Please set the OPENAI_API_KEY environment variable.")
    exit()

@function_tool
def get_current_time(timezone: str = "UTC") -> str:
    """Returns the current time in a specified timezone.

    Args:
        timezone: The timezone to get the time for (e.g., "America/New_York", "UTC"). Defaults to "UTC".

    Returns:
        A string representing the current time.
    """
    from datetime import datetime
    import pytz
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz)
        return now.strftime("%Y-%m-%d %H:%M:%S %Z%z")
    except pytz.UnknownTimeZoneError:
        return f"Error: Unknown timezone '{timezone}'."

agent = Agent(
    name="TimeAgent",
    instructions="You are a helpful assistant that can tell the current time.",
    tools=[get_current_time]
)

print("Running TimeAgent...")
result = Runner.run_sync(agent, "What time is it in New York?")

print("\n--- RunResult Inspection ---")
print(f"Final Output: {result.final_output}")
print(f"Original Input: {result.input}")
print(f"Last Agent: {result.last_agent.name}")

print("\nNew Items (Chronological Events):")
for item in result.new_items:
    print(f"  - Type: {type(item).__name__}")
    if hasattr(item, 'text'):
        print(f"    Text: {item.text[:50]}...") # Print first 50 chars of text
    if hasattr(item, 'tool_name'):
        print(f"    Tool Called: {item.tool_name}")
        print(f"    Tool Args: {item.tool_args}")
    if hasattr(item, 'output'):
        print(f"    Tool Output: {item.output}")

```

---

## Implementing Streaming for Enhanced User Experience

Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses. To stream, you can call Runner.run_streamed(), which will give you a RunResult.

While `RunResult` provides the complete picture after execution, waiting for the entire process to finish can lead to perceived latency, especially for complex tasks. **Streaming** allows you to receive partial outputs and events as they happen, providing real-time feedback to the user.

The `Runner.run_streamed()` method is designed for this purpose. Instead of returning a single `RunResult` at the end, it returns an iterable `RunResultStreaming` object that yields `RunItem` events as the agent progresses.

### When to Use Streaming:

*   **Chat Interfaces:** Displaying text token by token as the LLM generates it.
*   **Progress Indicators:** Showing tool calls, reasoning steps, or other intermediate events.
*   **Debugging:** Gaining real-time insight into the agent's execution flow.

### Example: Agent with Streaming Output

```python
import asyncio # Added
from agents import Agent, Runner
import os

# Ensure the OpenAI API key is set
# os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY" 

if "OPENAI_API_KEY" not in os.environ:
    print("Please set the OPENAI_API_KEY environment variable.")
    exit()

@function_tool
def how_many_jokes() -> int:
    import random

    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="StreamingAssistant",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(agent, input="Hello")
    print("=== Streaming started ===")

    async for event in result.stream_events():
        # Ignore raw token events
        if event.type == "raw_response_event":
            continue

        # Agent update events
        elif event.type == "agent_updated_stream_event":
            print(f"[Agent Updated] New agent: {event.new_agent.name}")
            continue

        # Item-level events
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(
                    f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}"
                )
            else:
                pass  # Ignore other event types

    


if __name__ == "__main__":
    asyncio.run(main())

```

**Explanation:**

*   The `with Runner.run_streamed(...) as stream:` block ensures the stream is properly managed.
*   The `for event in stream:` loop iterates over the `RunItem` objects yielded by the stream.
*   By checking `hasattr(event, 'text')`, we can print the LLM's generated text as it arrives, creating a typewriter effect.
*   You can add logic to handle other `RunItem` types (like `ToolCallItem` or `ToolCallOutputItem`) to provide even richer real-time feedback.
*   `stream.get_final_result()` allows you to retrieve the complete `RunResult` object after the streaming is finished.

### Handling Raw Responses Stream Events

Beyond the higher-level `RunItem` events, you might encounter `RawResponsesStreamEvent`. These are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like `response.created`, `response.output_text.delta`, etc.) and data. These events are useful if you want to stream raw, token-level responses directly from the model, giving you the most granular control over the output.

---

## Key Takeaways

*   The `RunResult` object is a comprehensive record of an agent's execution, providing insights into its `final_output`, `new_items` (events), and more.
*   `Runner.run_streamed()` enables real-time delivery of agent outputs and events, significantly improving user experience in interactive applications.
*   By combining `RunResult` inspection with streaming, you gain powerful capabilities for debugging, monitoring, and building dynamic agent interfaces.

